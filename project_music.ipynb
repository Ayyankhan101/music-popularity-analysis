{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad1b245f",
   "metadata": {},
   "source": [
    "# Music Data Analysis: A Deep Dive into Song Characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2ef066",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "In this notebook, we will conduct a comprehensive exploratory data analysis (EDA) of a music dataset. Our primary goals are to understand the factors that contribute to a song's popularity, explore the relationships between different audio features, and build a simple model to predict popularity. We will cover the following steps:\n",
    "\n",
    "*   **Data Loading and Cleaning:** We'll start by loading the dataset and preparing it for analysis by handling missing values, duplicates, and irrelevant columns.\n",
    "*   **Exploratory Data Analysis (EDA):** We'll use visualizations to explore the distributions of key variables, analyze the characteristics of different genres, and uncover correlations between audio features.\n",
    "*   **Predictive Modeling:** We'll build a linear regression model to predict song popularity based on its audio features and evaluate its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beacd856",
   "metadata": {},
   "source": [
    "### 1.1. Importing Libraries\n",
    "\n",
    "First, let's import the necessary libraries for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acab8f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07268e7",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Cleaning\n",
    "\n",
    "Now, we'll load the dataset and perform some initial cleaning to ensure our data is ready for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8d428a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 15, saw 2\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mParserError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdataset.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlatin1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/jupyter_venv/jupyter_venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/jupyter_venv/jupyter_venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/jupyter_venv/jupyter_venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1916\u001b[39m nrows = validate_integer(\u001b[33m\"\u001b[39m\u001b[33mnrows\u001b[39m\u001b[33m\"\u001b[39m, nrows)\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/jupyter_venv/jupyter_venv/lib/python3.13/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[39m, in \u001b[36mCParserWrapper.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.low_memory:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m         chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[32m    236\u001b[39m         data = _concatenate_chunks(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:838\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.read_low_memory\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:905\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._read_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:2061\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mParserError\u001b[39m: Error tokenizing data. C error: Expected 1 fields in line 15, saw 2\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('dataset.csv', encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b1af94",
   "metadata": {},
   "source": [
    "### 2.1. Initial Data Inspection\n",
    "\n",
    "Let's start by looking at the first few rows of the dataframe, its shape, and some basic information about the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2fa4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4e05d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a62e752",
   "metadata": {},
   "source": [
    "### 2.2. Cleaning the Data\n",
    "\n",
    "From the initial inspection, we can see an `Unnamed: 0` column, which appears to be an index. We'll drop this column as it's redundant. We also need to check for any missing values and duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961cdf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the unnecessary column\n",
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "# Check for missing values\n",
    "print('Missing values:', df.isnull().sum())\n",
    "\n",
    "# Check for duplicate tracks\n",
    "print('Number of duplicate tracks:', df.duplicated(subset=['track_id']).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06fae7b",
   "metadata": {},
   "source": [
    "We have a few missing values and some duplicate tracks. Let's remove the duplicates and the rows with missing values to clean up our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a68f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates and missing values\n",
    "df.dropna(inplace=True)\n",
    "df.drop_duplicates(subset=['track_id'], inplace=True)\n",
    "\n",
    "# Verify the cleaning\n",
    "print('Missing values after cleaning:', df.isnull().sum().sum())\n",
    "print('Number of duplicate tracks after cleaning:', df.duplicated(subset=['track_id']).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8e0caf",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)\n",
    "\n",
    "With our data cleaned, we can now dive into the exploratory analysis. We'll start by looking at the distribution of song popularity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b81f6f",
   "metadata": {},
   "source": [
    "### 3.1. Distribution of Popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa6c5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['popularity'], bins=30, kde=True)\n",
    "plt.title('Distribution of Song Popularity')\n",
    "plt.xlabel('Popularity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d818a3a8",
   "metadata": {},
   "source": [
    "The popularity score is fairly evenly distributed, with a slight skew towards less popular songs. Now, let's see which genres are the most popular."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8166ef",
   "metadata": {},
   "source": [
    "### 3.2. Top 10 Most Popular Genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fccdad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_genres = df.groupby('track_genre')['popularity'].mean().sort_values(ascending=False).head(10)\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.barplot(x=top_genres.values, y=top_genres.index, palette='viridis')\n",
    "plt.title('Top 10 Most Popular Genres')\n",
    "plt.xlabel('Average Popularity')\n",
    "plt.ylabel('Genre')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e209ffa0",
   "metadata": {},
   "source": [
    "Pop, rock, and dance-related genres seem to dominate the top of the popularity charts. This gives us a good idea of what kind of music is generally popular.\n",
    "\n",
    "Next, let's examine the correlation between the different audio features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c87519d",
   "metadata": {},
   "source": [
    "### 3.3. Correlation Matrix of Audio Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7965977d",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_features = ['popularity', 'danceability', 'energy', 'loudness', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo']\n",
    "correlation_matrix = df[audio_features].corr()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='viridis', fmt='.2f')\n",
    "plt.title('Correlation Matrix of Audio Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68f8059",
   "metadata": {},
   "source": [
    "From the heatmap, we can see some interesting correlations:\n",
    "\n",
    "*   `energy` and `loudness` are strongly positively correlated, which makes sense as high-energy songs are often louder.\n",
    "*   `acousticness` and `energy` are strongly negatively correlated, meaning acoustic tracks tend to be lower in energy.\n",
    "*   `popularity` has a moderate positive correlation with `loudness` and a slight positive correlation with `energy`. This suggests that louder, more energetic songs tend to be more popular."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db55a3ed",
   "metadata": {},
   "source": [
    "## 4. Predictive Modeling\n",
    "\n",
    "Now, let's build a simple linear regression model to predict a song's popularity based on its audio features. This will help us understand how much of a song's popularity can be explained by these features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a58572",
   "metadata": {},
   "source": [
    "### 4.1. Feature Selection and Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ea8706",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['danceability', 'energy', 'loudness', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo']\n",
    "X = df[features]\n",
    "y = df['popularity']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9f0c94",
   "metadata": {},
   "source": [
    "### 4.2. Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aed48bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc33a13",
   "metadata": {},
   "source": [
    "### 4.3. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3959e51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse:.2f}')\n",
    "print(f'R-squared: {r2:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab0998c",
   "metadata": {},
   "source": [
    "The R-squared value is quite low, which indicates that our model doesn't explain much of the variance in song popularity. This suggests that while audio features have some influence, other factors not included in our model (like artist fame, marketing, and cultural trends) play a much larger role in determining a song's popularity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ff983e",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "In this analysis, we cleaned the music dataset, explored the relationships between various audio features, and built a simple predictive model. Our key findings include:\n",
    "\n",
    "*   **Data Quality:** The initial dataset was relatively clean but required some preprocessing to handle duplicates and irrelevant columns.\n",
    "*   **Popular Music Trends:** Popular songs tend to be higher in energy and loudness, and often fall into genres like pop, rock, and dance.\n",
    "*   **Feature Relationships:** We confirmed intuitive relationships between audio features, such as the strong positive correlation between energy and loudness.\n",
    "*   **Predictive Power:** While audio features have some predictive power, they are not sufficient to fully explain song popularity, highlighting the importance of external factors.\n",
    "\n",
    "This analysis provides a solid foundation for further investigation. Future work could involve incorporating more features (such as artist information or album data), exploring more complex models, or analyzing trends over time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_venv",
   "language": "python",
   "name": "jupyter_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
